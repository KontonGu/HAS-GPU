apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "convnext-server"
spec:
  predictor:
    minReplicas: 1      # Optional: minimum replicas
    maxReplicas: 2      # Maximum replicas
    scaleTarget: 1
    #scaleMetric: concurrency
    containerConcurrency: 15
    # model:
    #   modelFormat:
    #     name: pytorch
    #   runtime: custom
    containers:
      - name: convnext-large
        image: leslie233/convnext:fast-test
        ports:
          - containerPort: 8080
        # resources:
        #   requests:
        #     cpu: 4
        #     memory: 6Gi
        #   limits:
        #     cpu: 20
        #     memory: 15Gi
        readinessProbe:
          tcpSocket:
            port: 5000  # Use the port your container is actually listening on
          initialDelaySeconds: 5  # Increase this for large models that take time to load
          periodSeconds: 3
          timeoutSeconds: 2
          successThreshold: 1
          failureThreshold: 3
        
        volumeMounts:
          - name: "model-volume"
            mountPath: "/models/"
    #     # Optional: you can specify environment variables if needed
    #     #env:
    #       # - name: MODEL_NAME
    #       #   value: "my-model"
    #     # Optional: set ports if your container uses non-default ports
    #     # ports:
    #     #   - containerPort: 8080
    # volumes:
    #   - name: "model-volume"
    #     hostPath:
    #       path: "/models/"
    volumes:
      - name: "model-volume"
        persistentVolumeClaim:
          claimName: models-pvc
